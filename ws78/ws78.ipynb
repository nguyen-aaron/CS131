import csv
import os
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import avg
import matplotlib.pyplot as plt


spark = SparkSession.builder.appName("SparkSQLApp").getOrCreate()

def find_average(file):
    path = f"gs://dataproc-staging-us-central1-359639680738-vfiiktvj/data/{file}"
    
    df = (spark.read.format("csv")
    .option("inferSchema", "true")
    .option("header", "true")
    .load(path))
    
    start = time.time()
    df.groupBy("day").agg(avg("degree")).show()
    end = time.time()
    
    print("Time for " + file + ": " + str(end - start))
    return (end - start)

times = []
for file in ["data100.csv", "data200.csv", "data400.csv", "data800.csv", "data1600.csv"]:
    runtime = find_average(file)
    times.append(runtime)


sizes = [100, 200, 400, 800, 1600]

plt.figure(figsize=(10, 6))
plt.plot(sizes, times, marker='o')
plt.title("Spark Runtime vs File Size")
plt.xlabel("Number of Rows")
plt.ylabel("Runtime (seconds)")
plt.grid(True)
plt.tight_layout()
plt.show()
